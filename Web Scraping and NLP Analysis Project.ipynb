{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb816aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the input file\n",
    "df = pd.read_excel(\"input.xlsx\")\n",
    "\n",
    "# Iterating over the URLs\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Sendinng a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Creating a BeautifulSoup object\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Finding the article title\n",
    "        title_element = soup.find('h1', class_='entry-title')\n",
    "        article_title = title_element.text.strip() if title_element else ''\n",
    "        \n",
    "        # Finding the article text\n",
    "        content_element = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "        article_text = content_element.text.strip() if content_element else ''\n",
    "        \n",
    "        # Saving the extracted article to a text file\n",
    "        filename = f\"{url_id}.txt\"\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(article_title + '\\n\\n')\n",
    "            file.write(article_text)\n",
    "        \n",
    "        print(f\"Article {url_id} saved successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve article {url_id}. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ada8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import textstat\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Function for stemming and removing stopwords\n",
    "def stemming(content):\n",
    "    port_stem = PorterStemmer()\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word.lower() not in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "\n",
    "# Reading positive words from file\n",
    "positive_words = set()\n",
    "with open('positive-words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        positive_words.add(line.strip().lower())\n",
    "\n",
    "# Reading negative words from file\n",
    "negative_words = set()\n",
    "with open('negative-words.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        negative_words.add(line.strip().lower())\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "input_directory = os.path.join(current_directory, \"New content\")\n",
    "\n",
    "# Output file path\n",
    "output_file = 'C:/Users/91821/Desktop/ML related/output file/Output.csv'\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterating over the text files\n",
    "for i in range(38, 151):\n",
    "    file_path = os.path.join(input_directory, f\"{i}.txt\")\n",
    "    url_id = i\n",
    "\n",
    "    try:\n",
    "        # Reading the content of the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            extracted_data = file.read()\n",
    "\n",
    "        # Tokenizing the extracted data into sentences\n",
    "        sentences = sent_tokenize(extracted_data)\n",
    "\n",
    "        # Applying stemming and removing stopwords on sentences\n",
    "        sentences = [stemming(sentence) for sentence in sentences]\n",
    "\n",
    "        # Text Analysis\n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        polarity_score = 0\n",
    "        subjectivity_score = 0\n",
    "        avg_sentence_length = 0\n",
    "        percentage_complex_words = 0\n",
    "        fog_index = 0\n",
    "        avg_words_per_sentence = 0\n",
    "        complex_word_count = 0\n",
    "        word_count = 0\n",
    "        syllables_per_word = 0\n",
    "        personal_pronouns = 0\n",
    "        avg_word_length = 0\n",
    "        cleaned_words = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            cleaned_words.extend(sentence.split())\n",
    "\n",
    "        word_count = len(cleaned_words)\n",
    "\n",
    "        # Calculating parameters\n",
    "        for word in cleaned_words:\n",
    "            # Sentiment analysis\n",
    "            if word.lower() in positive_words:\n",
    "                positive_score += 1\n",
    "            elif word.lower() in negative_words:\n",
    "                negative_score += 1\n",
    "\n",
    "            # Counting complex words\n",
    "            if textstat.syllable_count(word) > 2:\n",
    "                complex_word_count += 1\n",
    "\n",
    "            # Counting syllables\n",
    "            syllables_per_word += textstat.syllable_count(word)\n",
    "\n",
    "            # Counting personal pronouns\n",
    "            if word.lower() in ['i', 'we', 'my', 'mine', 'us', 'our', 'ours']:\n",
    "                personal_pronouns += 1\n",
    "\n",
    "        # Calculating polarity score\n",
    "        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "        # Calculating subjectivity score\n",
    "        subjectivity_score = (positive_score + negative_score) / (word_count + 0.000001)\n",
    "\n",
    "        # Calculating average sentence length\n",
    "        avg_sentence_length = word_count / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "        # Calculating percentage of complex words\n",
    "        percentage_complex_words = (complex_word_count / word_count) * 100 if word_count > 0 else 0\n",
    "\n",
    "        # Calculating fog index\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "        # Calculating average number of words per sentence\n",
    "        avg_words_per_sentence = word_count / len(sentences) if len(sentences) > 0 else 0\n",
    "\n",
    "        # Calculating average word length\n",
    "        if word_count > 0:\n",
    "            avg_word_length = sum(len(word) for word in cleaned_words) / word_count\n",
    "\n",
    "        # Storing the results in a dictionary\n",
    "        result = {\n",
    "            'URL_ID': url_id,\n",
    "            'Positive Score': positive_score,\n",
    "            'Negative Score': negative_score,\n",
    "            'Polarity Score': polarity_score,\n",
    "            'Subjectivity Score': subjectivity_score,\n",
    "            'Average Sentence Length': avg_sentence_length,\n",
    "            'Percentage of Complex Words': percentage_complex_words,\n",
    "            'FOG Index': fog_index,\n",
    "            'Average Number of Words per Sentence': avg_words_per_sentence,\n",
    "            'Complex Word Count': complex_word_count,\n",
    "            'Word Count': word_count,\n",
    "            'Syllables per Word': syllables_per_word,\n",
    "            'Personal Pronouns': personal_pronouns,\n",
    "            'Average Word Length': avg_word_length\n",
    "        }\n",
    "\n",
    "        # Appending the result to the list\n",
    "        results.append(result)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "\n",
    "# Write the output data to the output file\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=results[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(\"Output file saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
